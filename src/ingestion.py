import os
from sec_edgar_downloader import Downloader
from langchain_community.document_loaders import BSHTMLLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import shutil

# --- é…ç½®éƒ¨åˆ† ---
DATA_DIR = "data"
SEC_DIR = os.path.join(DATA_DIR, "sec_filings")
DB_DIR = os.path.join(DATA_DIR, "vector_db")

# SEC è¦æ±‚ä¸‹è½½æ—¶æä¾› User-Agent (æ ¼å¼: "Name email@domain.com")
USER_EMAIL = os.getenv("SEC_EMAIL", "student@university.edu") 
USER_NAME = "FinanceAgentProject"

def download_reports(tickers):
    """
    ä» SEC EDGAR ä¸‹è½½ 10-K (å¹´æŠ¥) å’Œ 10-Q (å­£æŠ¥)
    """
    print(f"ğŸš€ åˆå§‹åŒ–ä¸‹è½½å™¨... (User: {USER_NAME} <{USER_EMAIL}>)")
    # æ³¨æ„ï¼šv5.x ç‰ˆæœ¬çš„ sec-edgar-downloader ä¼šåœ¨ SEC_DIR ä¸‹åˆ›å»º 'sec-edgar-filings' æ–‡ä»¶å¤¹
    dl = Downloader(USER_NAME, USER_EMAIL, SEC_DIR)
    
    for ticker in tickers:
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½ {ticker} çš„è´¢æŠ¥...")
        # é™åˆ¶æ•°é‡ä»¥åŠ å¿«æ¼”ç¤ºé€Ÿåº¦
        dl.get("10-K", ticker, limit=1, download_details=True)
        dl.get("10-Q", ticker, limit=1, download_details=True)
    
    print("âœ… æ‰€æœ‰æ–‡ä»¶ä¸‹è½½å®Œæˆï¼")

def ingest_data():
    """
    è¯»å– HTML/HTM -> æ¸…æ´— -> åˆ‡å— -> å‘é‡åŒ– -> å­˜å…¥ ChromaDB
    """
    # æ£€æŸ¥ä¸‹è½½è·¯å¾„æ˜¯å¦å­˜åœ¨
    # sec-edgar-downloader é€šå¸¸ä¼šç”Ÿæˆè¿™ä¸ªå­ç›®å½•
    search_path = os.path.join(SEC_DIR, "sec-edgar-filings")
    if not os.path.exists(search_path):
        search_path = SEC_DIR
    
    print(f"ğŸ” æ­£åœ¨æœç´¢ç›®å½•: {search_path}")

    # 1. åŠ è½½æ•°æ® - ä¼˜åŒ– glob ä»¥åŒ…å« .htm å’Œ .html
    print("ğŸ“‚ å¼€å§‹åŠ è½½è´¢æŠ¥æ–‡ä»¶...")
    
    # å°è¯•åŠ è½½å¤šç§åç¼€
    docs = []
    for ext in ["**/*.html", "**/*.htm"]:
        loader = DirectoryLoader(
            search_path,
            glob=ext,
            loader_cls=BSHTMLLoader,
            show_progress=True,
            use_multithreading=True
        )
        loaded_docs = loader.load()
        docs.extend(loaded_docs)
        print(f"   - åŒ¹é… {ext}: æ‰¾åˆ° {len(loaded_docs)} ä¸ªæ–‡æ¡£")

    print(f"ğŸ“„ æ€»è®¡åŠ è½½äº† {len(docs)} ä¸ªæœ‰æ•ˆæ–‡æ¡£")

    if len(docs) == 0:
        print("âš ï¸ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½• HTML/HTM æ–‡æ¡£ã€‚")
        print("ğŸ’¡ è¯·æ£€æŸ¥ data/sec_filings ç›®å½•ä¸‹æ˜¯å¦æœ‰æ–‡ä»¶ã€‚æœ‰äº›æ—§è´¢æŠ¥å¯èƒ½æ˜¯ .txt æ ¼å¼ã€‚")
        return

    # 2. æ–‡æœ¬åˆ‡å— (Chunking)
    print("âœ‚ï¸ æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=150, # ç•¥å¾®å¢åŠ é‡å åº¦ä»¥ä¿æŒä¸Šä¸‹æ–‡
        separators=["\n\n", "\n", " ", ""]
    )
    splits = text_splitter.split_documents(docs)
    print(f"ğŸ§© ç”Ÿæˆäº† {len(splits)} ä¸ªæ–‡æœ¬å— (Chunks)")

    # 3. å‘é‡åŒ–ä¸å­˜å‚¨
    print("ğŸ’¾ æ­£åœ¨ç”Ÿæˆå‘é‡å¹¶å­˜å…¥ ChromaDB...")
    
    # ä½¿ç”¨ BGE-Small æ¨¡å‹
    embedding_model = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-en-v1.5",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    # æ¸…ç†æ—§çš„æ•°æ®åº“ä»¥é˜²å†²çª (å¯é€‰)
    # if os.path.exists(DB_DIR):
    #     shutil.rmtree(DB_DIR)
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embedding_model,
        persist_directory=DB_DIR
    )
    print(f"âœ… å‘é‡æ•°æ®åº“æ„å»ºæˆåŠŸï¼å­˜å‚¨ä½ç½®: {DB_DIR}")

if __name__ == "__main__":
    target_tickers = ["AAPL", "MSFT", "TSLA"]
    
    # å¦‚æœå·²ç»ä¸‹è½½è¿‡ï¼Œå¯ä»¥å°† RUN_DOWNLOAD è®¾ä¸º False ä»¥èŠ‚çœæ—¶é—´
    RUN_DOWNLOAD = True 
    
    if RUN_DOWNLOAD:
        download_reports(target_tickers)
    
    ingest_data()